{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f460238b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (760429627.py, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/pt/3m3r1rq55pdgzpmnqzkdjb240000gn/T/ipykernel_6883/760429627.py\"\u001b[0;36m, line \u001b[0;32m20\u001b[0m\n\u001b[0;31m    The purpose of this project is to assist in the prediction of log error for Zillow's Zestimate house value predictions. This will be done by:\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Dataframe manipulations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualizations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Modules needed to perform necessary functions\n",
    "import wrangle_zillow as w\n",
    "import explore as e\n",
    "\n",
    "# Turning off warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "## Overview:\n",
    "The purpose of this project is to assist in the prediction of log error for Zillow's Zestimate house value predictions. This will be done by:\n",
    "- Identifying some of the key drivers behind the log error\n",
    "- Applying these insights to regression models that can help predict the log error\n",
    "- Sharing learned insights to provide recommendations and solutions moving forward \n",
    "\n",
    "### Planning:\n",
    "Prior to interacting with the data we want to lay out some of our intentions/initial questions:\n",
    "\n",
    "Some of the initial questions for the data: \n",
    "1. Do primary house attributes impact log error? (bedrooms, bathrooms, age, squarefeet)\n",
    "2. Do secondary house attributes impact log error? (num_fireplace, threequarter_baths, hottub_or_spa, has_pool)\n",
    "3. Does geography impact log error? (latitude, longitude, regionidzip, fips)\n",
    "4. Can we successfully use any of our features to cluster for log error predictions?\n",
    "    - Geographic clustering\n",
    "        - Latitude/Longitude\n",
    "    - Continuous feature clustering\n",
    "5. What can we identify about the data when log error is postive or negative?\n",
    "\n",
    "Some of the hypotheses to be explored:\n",
    "1. Is there a linear relationship between log error and our continuous features? (Pearsonr)\n",
    "2. Is there a difference in the mean log error for selected subsets and the entire dataset? (one-sample t-test)\n",
    "3. Is there a difference in the mean log error of particular zip codes and the entire dataset? (one-sample t-test)\n",
    "\n",
    "### Acquire:\n",
    "The wrangle_zillow.py module contains the functions used to acquire our data. The get_db_url() function assists in accessing the SQL server and then using a query and the acquire_zillow() function we gather the necessary data and store it in a dataframe. Our initial dataframe contains a number of columns that will be narrowed down through preparation and exploration of the data. \n",
    "\n",
    "# In our wrangle_zillow() module we use our acquire_zillow() function to gather the Zillow data from the SQL server\n",
    "zillow = w.acquire_zillow()\n",
    "\n",
    "### Prepare:\n",
    "After acquiring our data we will need to do a fair bit of modification and or manipulation to make it wholly useful for our purposes. The following are the steps that were taken:\n",
    "1. Ensuring we are only working with single unit properties, utilizing identifiers from the SQL server\n",
    "2. Identifying a lack of proper data input for some columns and filling the nulls to signify better inputs\n",
    "    - Main data this was utilized for: fireplace, hottub/spa, pool, three quarter bath, tax delinquency\n",
    "3. Dropping leftover null values, and unwanted data (based on unusable or incorrect data inputs)\n",
    "    - This is done with all encompassing mechanisms (dropna()), non-null proportion requirements by row or column, and eliminating faulty data inputs (e.g. 0 bedrooms)\n",
    "4. Feature engineering age from yearbuilt data\n",
    "5. Ensuring columns are the correct data type\n",
    "6. Removal of outliers to make our outcomes as generally usable as possible\n",
    "7. Encoding our currently recognized categorical columns\n",
    "    - fips, hottub_or_spa, has_pool, tax_delinquency\n",
    "8. Renaming our columns for easier use\n",
    "9. Splitting our original dataframe into train, validate, and test dataframes\n",
    "\n",
    "# In our wrangle_zillow() module we use our prepare_zillow() function, with our acquired dataframe\n",
    "# We clean, prepare, and split the dataframe to produce train, validate, and test dataframes\n",
    "train, validate, test = w.prepare_zillow(zillow)\n",
    "\n",
    "### Explore:\n",
    "...\n",
    "\n",
    "# We will set our alpha for all of our statistical testing\n",
    "alpha = .05\n",
    "\n",
    "To address Question 5 (log error being positive or negative) throughout the exploratory phase we can split our train into positive and negative dataframes.\n",
    "\n",
    "train_pos = train[train.logerror >= 0]\n",
    "train_neg = train[train.logerror < 0]\n",
    "\n",
    "#### Question 1: Do primary house attributes impact log error?\n",
    "Most of our house attributes are continuous features so we will analyze them all together using some functions in our explore.py module. The linear_tests() function will provide some visualizations and a pearsonr test to describe the relationship between our continuous feature and the target variable. \n",
    "\n",
    "From our exploration we visually and statistically confirm the use of all of our continuous features.\n",
    "\n",
    "# We need to establish our target feature\n",
    "target = 'logerror'\n",
    "\n",
    "# We can demonstrate continuous feature exploration using our 'age' feature\n",
    "# For positive log error\n",
    "e.linear_tests(train_pos, ['age'], target)\n",
    "\n",
    "# With our negative log error\n",
    "e.linear_tests(train_neg, ['age'], target)\n",
    "\n",
    "#### Question 2: Do secondary house attributes impact log error?\n",
    "Our secondary house attributes were largely categorical and were explored using visualizations and a one-sample t-test. The cat_visuals() function in the explore.py module can be used for visualizations, and the explore notebook can be viewed to see the statistical analysis. \n",
    "\n",
    "From our exploration we visually and statistically confirm the use of has_pool and tax_delinquency, but will not be using fips or hottub_or_spa in modeling. \n",
    "\n",
    "# We can demonstrate categorical feature exploration using our 'tax_delinquency' feature\n",
    "# For positive log error\n",
    "e.cat_visuals(train_pos, ['tax_delinquency'], target)\n",
    "\n",
    "# For negative log error\n",
    "e.cat_visuals(train_neg, ['tax_delinquency'], target)\n",
    "\n",
    "Moving forward our exploration digs deeper into clustering work. Prior to clustering a good overview of how little correlation there is currently with our features and our target variable. The heatmap_zillow() function can provide this visual.\n",
    "\n",
    "# Using heatmap_zillow() to see if there are any strong correlations with our target variable\n",
    "e.heatmap_zillow(train)\n",
    "\n",
    "There is a very weak correlation with age, but nothing in particular has a correlation to log error. We will still be using the features that were deemed significant through statistical tests. \n",
    "\n",
    "### Scaling:\n",
    "Prior to clustering the data we need to scale our continuous features that will be used. To do this we will use our scale_zillow() function that resides in our explore.py module. It will take our train, validate, and test dataframes and return them with scaled features. \n",
    "\n",
    "# Selecting our features to scale\n",
    "quants = ['bathrooms', 'bedrooms', 'squarefeet', 'num_fireplace', 'latitude',\n",
    "          'longitude', 'threequarter_baths', 'age']\n",
    "\n",
    "# Scaling our dataframes: train, validate, test\n",
    "train, validate, test = e.scale_zillow(train, validate, test, quants)\n",
    "\n",
    "### Clustering:\n",
    "#### Question 3: Does geography impact log error?\n",
    "To try to incorporate geographical data we will cluster using latitude and longitude. We will also conduct visualization and statistical testing for our regionidzip feature. \n",
    "\n",
    "For our clustering we will conduct an elbow test to determine the k value for number of clusters, using the elbow_test() function. We will then use the clustering_cols() function to create our clusters and apply the transformation to our train, validate, and test dataframes. Both of these functions are in the explore module.\n",
    "\n",
    "# Selection of features for clustering\n",
    "col1 = ['latitude', 'longitude']\n",
    "\n",
    "# Use of elbow_test() to find best k value\n",
    "e.elbow_test(train, col1)\n",
    "\n",
    "# Use of clustering_cols() function to find our centroids and add our cluster column to train, validate, and test\n",
    "centroids, train, validate, test = e.clustering_cols(train, validate, test, col1, 6, 'lat_long_cluster')\n",
    "\n",
    "We can map our clusters using a relplot.\n",
    "\n",
    "# Visualizing our clusters and centroids\n",
    "sns.relplot(data=train, x='latitude', y='longitude', hue='lat_long_cluster', aspect=1.5)\n",
    "plt.title('Clustering by latitude and longitude')\n",
    "plt.scatter(centroids.latitude, centroids.longitude, marker='x', s=600, c='red')\n",
    "\n",
    "##### Regionidzip:\n",
    "Through visualizations and statistical testing (one-sample t-test) a list of statistically significant zip codes were selected to move forward with for modeling. This will be addressed in the same manner as selecting which clusters are most significant. \n",
    "\n",
    "#### Question 4: Can we successfully use any of our features to cluster for log error predictions? \n",
    "Using the same functions as above we analyze clusters for some of our other features.\n",
    "\n",
    "# Curating our other selected features to cluster\n",
    "col2 = ['bathrooms', 'bedrooms', 'squarefeet', 'age']\n",
    "col3 = ['age', 'squarefeet']\n",
    "\n",
    "# Calling our clustering_cols() for col2 and col3\n",
    "# The elbow_test() results can be seen in the explore notebook\n",
    "centroids, train, validate, test = e.clustering_cols(train, validate, test, col2, 8, 'conts_cluster')\n",
    "centroids2, train, validate, test = e.clustering_cols(train, validate, test, col3, 4, 'age_sqft_cluster')\n",
    "\n",
    "# A visualization of our 'age_sqft_cluster'\n",
    "sns.relplot(data=train, x='age', y='squarefeet', hue='age_sqft_cluster', aspect=1.5)\n",
    "plt.title('Clustering by sqft and age')\n",
    "plt.scatter(centroids2.age, centroids2.squarefeet, marker='x', s=600, c='red')\n",
    "\n",
    "#### Question 5: What can we identify about the data when log error is positive or negative? \n",
    "The log error being positive or negative was explored through visualizations with our continuous and categorical variables. There is a worthwhile pursuit to see if applying an absolute value to log error would yield better results for exploration and modeling, but that ultimately fell outside of the scope of this project and will be mentioned later in the summary.\n",
    "\n",
    "### Statistical testing for numerous encoded columns:\n",
    "Visualizations for these columns can be seen in the explore notebook. Statistical testing was done with the run_ttest_list() function to create lists of significant columns.\n",
    "\n",
    "# Establishing the overall_logerror_mean for testing\n",
    "overall_logerror_mean = train.logerror.mean()\n",
    "\n",
    "# List of significant clusters for lat_long_cluster\n",
    "lat_long_list = []\n",
    "for i, subset in train.groupby('lat_long_cluster'):\n",
    "    e.run_ttest_list(subset, lat_long_list, overall_logerror_mean, i)\n",
    "    \n",
    "# List of significant clusters for conts_cluster\n",
    "conts_cluster = []\n",
    "for i, subset in train.groupby('conts_cluster'):\n",
    "    e.run_ttest_list(subset, conts_cluster, overall_logerror_mean, i)\n",
    "    \n",
    "# List of significant clusters for age_sqft_cluster\n",
    "age_sqft = []\n",
    "for i, subset in train.groupby('age_sqft_cluster'):\n",
    "    e.run_ttest_list(subset, age_sqft, overall_logerror_mean, i)\n",
    "    \n",
    "# List of significant zip codes for regionidzip\n",
    "zip_list = []\n",
    "for i, subset in train.groupby('regionidzip'):\n",
    "    e.run_ttest_list(subset, zip_list, overall_logerror_mean, i)\n",
    "\n",
    "### Additional encoding:\n",
    "Our new cluster columns and regionidzip need to be encoded to represent the significant values.\n",
    "\n",
    "# Selection of our columns and encoding applied to train, validate, and test dataframes\n",
    "to_encode = ['conts_cluster', 'regionidzip', 'lat_long_cluster', 'age_sqft_cluster']\n",
    "for col in to_encode:\n",
    "    train = pd.get_dummies(train, columns=[col])\n",
    "    validate = pd.get_dummies(validate, columns=[col])\n",
    "    test = pd.get_dummies(test, columns=[col])\n",
    "\n",
    "### Trimming our dataframes:\n",
    "Before we move forward to modeling we will trim whatever features will not be used. The selection of these was determined through our visual and statistical exploration. \n",
    "\n",
    "# Columns that are redundant through new columns or were not deemed significant that can be dropped\n",
    "to_drop = ['latitude', 'longitude', 'fips_06059', 'fips_06111', 'fips_06037', 'hottub_or_spa']\n",
    "\n",
    "# Dropping them from our dataframes\n",
    "train = train.drop(columns=to_drop)\n",
    "validate = validate.drop(columns=to_drop)\n",
    "test = test.drop(columns=to_drop)\n",
    "\n",
    "Trimming our encoded cluster columns will utilize the lists that were created previously. The function cluster_trim() will loop through the train, validate, and test dataframes and remove the columns that were not deemed statistically significant.\n",
    "\n",
    "# Removing the unnecessary lat_long_cluster encoded columns\n",
    "train, validate, test = e.cluster_trim(lat_long_list, 'lat_long_cluster', train, validate, test)\n",
    "\n",
    "# Removing the unnecessary conts_cluster encoded columns\n",
    "train, validate, test = e.cluster_trim(conts_cluster, 'conts_cluster', train, validate, test)\n",
    "\n",
    "# Removing the unnecessary age_sqft_cluster encoded columns\n",
    "train, validate, test = e.cluster_trim(age_sqft, 'age_sqft', train, validate, test)\n",
    "\n",
    "# Removing the unnecessary age_sqft_cluster encoded columns\n",
    "train, validate, test = e.cluster_trim(zip_list, 'regionidzip', train, validate, test)\n",
    "\n",
    "### Exploration Summary:\n",
    "Through a number of visualizations, clustering, and statistical tests our selected features are:\n",
    "- bathrooms\n",
    "- bedrooms\n",
    "- squarefeet\n",
    "- num_fireplace\n",
    "- threequarter_baths\n",
    "- logerror (target)\n",
    "- age\n",
    "- has_pool \n",
    "- tax_delinquency\n",
    "- lat_long_cluster (cluster based on latitude and longitude)\n",
    "- conts_cluster (cluster based on bathrooms, bedrooms, squarefeet, and age)\n",
    "- age_sqft_cluster (cluster based on age and squarefeet)\n",
    "- regionidzip (39 unique encoded zip codes)\n",
    "\n",
    "Categorical features that were eliminated primarily through statistical testing:\n",
    "- fips\n",
    "- hottub_or_spa\n",
    "- Various encoded cluster and regionidzip columns\n",
    "\n",
    "Continuous features that were eliminated through engineering with clustering:\n",
    "- latitude\n",
    "- longitude\n",
    "\n",
    "In regards to the questions we asked of the data throughout the planning and exploration phases the big takeaways are:\n",
    "- The data we do have is not likely to be significantly useful, modeling will tell us more about this\n",
    "    - While the majority of our features are worth bringing forward to modeling none of them seemingly have a strong impcat on log error\n",
    "- Log error has similarities and differences in key drivers compared to the tax value/home price it originates from \n",
    "- Clustering allows for more nuanced approach to geography that would otherwise be difficult to bring to modeling\n",
    "\n",
    "### Modeling:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python397jvsc74a57bd038cca0c38332a56087b24af0bc80247f4fced29cb4f7f437d91dc159adec9c4e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
